\documentclass[12pt]{article}

% Packages
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{enumitem}

\hypersetup{
    colorlinks=true,
    linkcolor=blue!60!black,
    citecolor=blue!60!black,
    urlcolor=blue!60!black
}

\onehalfspacing

% Title
\title{\textbf{Learning the Structure of Partisan Conflict: \\
Graph Attention Networks and the Dynamics of Congressional Polarization}}

\author{
    Congressional GNN Research Group\thanks{Correspondence: Prepared using Voteview roll-call data from the 110th through 116th Congresses (2007--2021). All code and data are available upon request.}
}

\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
\noindent How does the structure of legislative voting reveal the dynamics of partisan conflict? We introduce CongressGAT, a temporal graph attention network that learns the evolving relational architecture of the U.S.\ House of Representatives from roll-call voting data spanning the 110th through 116th Congresses (2007--2021). The model constructs voting agreement graphs for each Congress, applies multi-head graph attention to learn which legislative relationships carry the most information, and uses temporal self-attention to track how these patterns shift across sessions. We evaluate on three tasks: coalition detection, party defection forecasting, and polarization measurement. On held-out data (115th--116th Congresses), the model achieves a coalition detection F1 of 0.99 and a defection prediction AUC of 0.80. The attention mechanism reveals that the model increasingly weights cross-party relationships in later, more polarized Congresses, suggesting it learns to identify the shrinking set of bipartisan connections as the most informative signals. A quasi-causal analysis confirms the Tea Party wave of 2010 as the single largest discrete shock to House polarization in our sample, producing a 0.25-point jump in our polarization index between the 111th and 112th Congresses.

\bigskip
\noindent\textit{Keywords:} graph neural networks, congressional polarization, legislative voting, attention mechanisms, temporal dynamics
\end{abstract}

\newpage
\tableofcontents
\newpage

% ============================================================
\section{Introduction}
% ============================================================

Start with a fact: if you randomly selected two House members from the 114th Congress, one Democrat and one Republican, they would have agreed on fewer than 29 percent of all recorded votes. In the 111th Congress, just three sessions earlier, that number was 55 percent. The change is enormous, but the raw statistic tells us nothing about \textit{how} that polarization operates in practice, which relationships carry the most weight, or what structural features of the voting network distinguish a polarized Congress from a less polarized one.

The standard tools for measuring polarization, DW-NOMINATE scores chief among them, project legislators onto a low-dimensional ideological space and measure the distance between party medians. This approach has been extraordinarily productive. But it treats each legislator as an independent point in space, discarding the relational structure that defines legislative behavior. A senator who votes identically to her party on every issue and one who arrives at the same voting record through a web of cross-party alliances and within-party defections look the same in an ideal-point model. They are not the same.

This paper asks whether a graph-based approach, one that takes the network of voting relationships as its primary object, can capture dimensions of polarization that point-estimate models miss. We introduce CongressGAT, a temporal graph attention network trained on Voteview roll-call data from the 110th through 116th Congresses. The model works in three stages. First, it constructs a voting agreement graph for each Congress, where edges between legislators are weighted by their rate of agreement on recorded votes. Second, it applies graph attention layers to learn which relationships in the network are most informative for predicting member behavior. Third, it uses temporal self-attention to track how these patterns evolve from one Congress to the next.

The approach yields three contributions. The first is methodological: we demonstrate that graph neural networks can learn meaningful structure from legislative voting data, achieving near-perfect coalition detection and meaningful defection forecasting on held-out Congresses. The second is interpretive: the attention mechanism provides a window into what the model actually learns, revealing that cross-party voting relationships become increasingly informative in more polarized sessions. The third is substantive: our analysis confirms the Tea Party wave of 2010 as the largest discrete polarization shock in the sample and identifies the 114th Congress (2015--2017) as a structural anomaly, with the highest polarization index but unusual spectral properties suggesting internal fragmentation within both parties.

A word about what this paper is not. We do not claim that graph neural networks should replace DW-NOMINATE or other established measures of legislative ideology. The existing tools work well for the questions they were designed to answer. Our argument is narrower: that the relational structure of voting networks contains information that scalar measures discard, and that attention-based graph models can extract some of that information in interpretable ways.

% ============================================================
\section{Background}
% ============================================================

\subsection{Congressional Polarization}

The polarization of the U.S. Congress has been among the most studied phenomena in American politics over the past three decades. The core empirical finding is not in dispute: the ideological distance between the median Democrat and the median Republican in both chambers has increased substantially since the 1970s, with particularly sharp acceleration after 1990 \citep{mccarty2006polarized}. DW-NOMINATE scores, estimated from roll-call voting matrices using a spatial model of voting, provide the standard measurement infrastructure for this finding \citep{poole1997congress}.

What remains contested is the mechanism. Some scholars emphasize elite-driven sorting, in which party leaders use procedural tools, particularly control of the legislative agenda, to force votes along party lines \citep{lee2009beyond}. Others point to constituent-level realignment, as the electorate itself sorts into ideologically coherent parties \citep{fiorina2005culture}. A third line of work highlights the role of primary elections and candidate selection in pulling nominees toward the extremes \citep{hall2015happens}. These accounts are not mutually exclusive, and our data cannot fully adjudicate between them. But they motivate different predictions about the \textit{structure} of voting networks, and this is where graph-based methods can add value.

\subsection{Graph Neural Networks in Social Science}

Graph neural networks (GNNs) generalize deep learning to non-Euclidean data structures. Where convolutional neural networks assume a regular grid (images) and recurrent networks assume a sequence (text), GNNs operate on graphs, where the connectivity itself is part of the input. The Graph Attention Network (GAT) variant, introduced by \citet{velickovic2018graph}, adds a self-attention mechanism that learns to weight different neighbors differently when aggregating information across the graph.

Applications of GNNs to political and social network data remain sparse. Most work has focused on social media networks \citep{wu2020comprehensive} or citation graphs \citep{kipf2017semi}. The closest precedent to our application is work using network analysis of roll-call votes to study partisan structure \citep{andris2015rise}, though that work uses descriptive network statistics rather than learned representations.

Our contribution is to combine three elements that have not, to our knowledge, been brought together in the legislative context: (1) per-congress graph construction from roll-call data, (2) attention-based graph representation learning, and (3) temporal modeling across multiple congressional sessions.

% ============================================================
\section{Data}
% ============================================================

We use Voteview roll-call data for the U.S. House of Representatives, spanning the 110th through 116th Congresses (January 2007 through January 2021) \citep{lewis2023voteview}. This period covers four presidential administrations (the latter years of George W. Bush, Barack Obama, and Donald Trump, plus the brief final session of the 116th under Trump) and several consequential political disruptions, including the Tea Party wave of 2010, the government shutdowns of 2013 and 2018--2019, and the onset of the Trump presidency.

Table~\ref{tab:data_summary} summarizes the data for each Congress. Each session includes approximately 440--455 House members and between 900 and 1,865 recorded roll-call votes. We restrict attention to substantive votes (cast codes 1--6 in the Voteview classification, excluding absences and non-votes) and to members serving in the House (excluding the President, whose votes appear in some Voteview files).

\begin{table}[H]
\centering
\caption{Data Summary by Congress}
\label{tab:data_summary}
\small
\begin{tabular}{lcccccc}
\toprule
Congress & Years & Members & Democrats & Republicans & Polarization & Defectors \\
\midrule
110th & 2007--09 & 453 & 246 & 207 & 0.520 & 80 (17.7\%) \\
111th & 2009--11 & 454 & 269 & 183 & 0.400 & 48 (10.6\%) \\
112th & 2011--13 & 445 & 200 & 245 & 0.647 & 89 (20.0\%) \\
113th & 2013--15 & 444 & 204 & 240 & 0.640 & 59 (13.3\%) \\
114th & 2015--17 & 441 & 190 & 251 & 0.690 & 39 (8.8\%) \\
115th & 2017--19 & 450 & 200 & 250 & 0.640 & 30 (6.7\%) \\
116th & 2019--21 & 451 & 241 & 208 & 0.642 & 59 (13.1\%) \\
\bottomrule
\end{tabular}
\medskip

\footnotesize{Polarization index measures $1 - (\text{between-party agreement} / \text{within-party agreement})$. Defectors are members whose cross-party voting rate exceeds 10\%. Data from Voteview \citep{lewis2023voteview}.}
\end{table}

Three patterns stand out in the raw data. First, polarization increases sharply from the 111th to the 112th Congress, jumping from 0.400 to 0.647, the largest single-session shift in the sample. This coincides precisely with the Tea Party wave election of 2010. Second, the number of defectors, members who vote against their party on more than 10 percent of roll calls, drops from 80 in the 110th Congress to just 30 in the 115th, consistent with the narrative of increasing party discipline. Third, the 114th Congress shows the highest polarization (0.690) and the fewest defectors (39), yet its spectral clustering alignment is anomalously low (0.544), meaning the graph's natural two-way partition does not cleanly separate parties. We return to this puzzle in Section 6.

\subsection{Graph Construction}

For each Congress, we construct a weighted, undirected voting agreement graph. Nodes represent House members. For each pair of members $(i, j)$, we compute the agreement rate:

\begin{equation}
    a_{ij} = \frac{\sum_{k} \mathbf{1}[v_{ik} = v_{jk}] \cdot \mathbf{1}[\text{both voted on } k]}{\sum_{k} \mathbf{1}[\text{both voted on } k]}
\end{equation}

\noindent where $v_{ik}$ is member $i$'s vote on roll call $k$ (coded as Yea or Nay, excluding absences). We place an edge between $i$ and $j$ when $a_{ij} > 0.5$, yielding graphs with approximately 50,000--90,000 edges per Congress. Edge weights are the raw agreement scores.

Node features comprise three variables: the member's first-dimension DW-NOMINATE score (a continuous measure of liberal-conservative ideology), the second-dimension score (capturing a secondary dimension often associated with social or cultural issues), and a party indicator (Democrat, Republican, or Independent).

\subsection{Label Construction}

We evaluate on three tasks. For \textit{coalition detection}, the label is simply the member's party affiliation (Democrat or Republican), excluding the handful of independents. For \textit{defection prediction}, we compute each member's defection rate as the fraction of votes on which they voted against the majority of their own party, and label as ``defectors'' those with rates exceeding 10 percent. For \textit{polarization prediction}, the target is the congress-level polarization index defined above.

% ============================================================
\section{Model}
% ============================================================

\subsection{Architecture}

CongressGAT consists of three components: a per-congress graph attention encoder, a temporal self-attention module, and task-specific prediction heads.

\paragraph{Graph Attention Encoder.} For each Congress, the encoder applies two GAT layers to learn node representations. The first layer uses four attention heads and maps the 3-dimensional input features to a 128-dimensional space (32 per head, concatenated). The second layer uses a single head and maps to a 32-dimensional embedding. The attention mechanism in each layer computes, for each pair of connected nodes $i$ and $j$:

\begin{equation}
    \alpha_{ij} = \frac{\exp\left(\text{LeakyReLU}(\mathbf{a}^T [\mathbf{W} h_i \| \mathbf{W} h_j])\right)}{\sum_{k \in \mathcal{N}(i)} \exp\left(\text{LeakyReLU}(\mathbf{a}^T [\mathbf{W} h_i \| \mathbf{W} h_k])\right)}
\end{equation}

\noindent where $\mathbf{W}$ is a learned linear transformation, $\mathbf{a}$ is a learned attention vector, $h_i$ is the feature vector of node $i$, and $\mathcal{N}(i)$ is the set of neighbors of $i$ in the agreement graph. The key property is that $\alpha_{ij}$ is \textit{learned}, not prescribed; the model discovers which neighbors matter most for each node.

After the second GAT layer, node embeddings are mean-pooled to produce a single graph-level embedding per Congress.

\paragraph{Temporal Self-Attention.} The sequence of graph-level embeddings (one per Congress) is passed through a multi-head self-attention layer \citep{vaswani2017attention}. This allows the model to learn temporal dependencies: for instance, that the 112th Congress is better understood in the context of the 111th (the pre-Tea Party baseline) than in the context of the 110th.

\paragraph{Prediction Heads.} Three separate two-layer feedforward networks produce predictions for each task: a regression head for polarization (outputting a scalar), a classification head for coalition detection (outputting class probabilities), and a binary head for defection prediction (outputting a defection probability per node).

\subsection{Training}

We train on the 110th through 114th Congresses and evaluate on the 115th and 116th, a proper temporal split that respects the sequential structure of the data. This is a stricter evaluation than cross-validation, as it requires the model to generalize forward in time.

The loss function is a weighted sum of mean squared error (polarization), cross-entropy (coalition), and binary cross-entropy (defection). We use the Adam optimizer with a learning rate of 0.005, weight decay of $10^{-4}$, and gradient clipping at norm 1.0. Training proceeds for up to 300 epochs with early stopping (patience of 30 epochs).

\subsection{Baselines}

We compare against three baselines. Logistic regression and random forest classifiers operate on the same node features enriched with two network-derived features: mean agreement with same-party members and mean agreement with cross-party members. For polarization prediction, we use a linear regression on congress-level aggregate features (mean and standard deviation of DW-NOMINATE scores, party ratio, mean agreement).

% ============================================================
\section{Results}
% ============================================================

\subsection{Coalition Detection}

The numbers here tell a straightforward story. All models achieve near-perfect coalition detection on the test set: CongressGAT obtains an F1 of 0.990, while logistic regression and random forest both score 1.000 (Table~\ref{tab:results}). This is not surprising. The DW-NOMINATE first dimension, which is an input feature for all models, almost perfectly separates Democrats from Republicans in the modern Congress. The result confirms that the graph structure is not needed for this particular task; the ideological coordinates do the work.

\begin{table}[H]
\centering
\caption{Model Performance: Train (110th--114th) vs.\ Test (115th--116th)}
\label{tab:results}
\small
\begin{tabular}{lccccc}
\toprule
& \multicolumn{2}{c}{Coalition F1} & \multicolumn{2}{c}{Defection AUC} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
Model & Train & Test & Train & Test \\
\midrule
CongressGAT & 0.895 & 0.990 & 0.662 & 0.799 \\
Logistic Regression & 0.998 & 1.000 & 0.907 & 0.984 \\
Random Forest & 0.999 & 1.000 & 0.939 & 0.986 \\
\bottomrule
\end{tabular}
\medskip

\footnotesize{Coalition detection uses macro F1 score. Defection prediction uses AUC-ROC. All models use the same node features (DW-NOMINATE dimensions and party); baselines additionally include network-derived agreement features.}
\end{table}

\subsection{Party Defection Prediction}

Defection prediction is the more revealing task. Here, the baselines outperform CongressGAT: random forest achieves a test AUC of 0.986 compared to the graph model's 0.799. This deserves a frank explanation. The baselines include two hand-engineered network features, mean same-party agreement and mean cross-party agreement, that directly encode the information most relevant to defection. These features are essentially a summary of each member's position in the agreement network. The GAT, in contrast, must learn to extract this information from the raw adjacency structure, a harder task with only three input features and the relatively small sample of congressional sessions.

The result highlights an important lesson about graph neural networks in this domain: when the relevant network information can be captured by simple summary statistics, there may be no advantage to learning the full graph structure. The graph model's value lies not in raw predictive performance but in its ability to reveal \textit{which} relationships the model attends to, information that summary statistics cannot provide.

That said, the GAT's performance on defection is non-trivial. An AUC of 0.80 on held-out data means the model correctly ranks a randomly chosen defector above a randomly chosen non-defector 80 percent of the time, using only three input features and the graph structure. For the 116th Congress specifically, the model achieves an AUC of 0.816. The performance is weakest for the 111th Congress (0.660), which is also the least polarized session in our sample, suggesting the model learns defection signals more effectively when partisan lines are sharper.

\subsection{Polarization Prediction}

The polarization regression yields a negative $R^2$ on the test set, meaning the model predicts worse than a constant mean. This is an inherent limitation of the setup: we have only seven data points (one per Congress), with five for training and two for testing. No model can reliably learn a regression function from five observations, and we report this result honestly rather than omitting it. The linear baseline fares even worse, with a test $R^2$ of $-33{,}989$. The polarization trajectory is better understood descriptively than predictively, and we turn to that description now.

\subsection{Polarization Dynamics}

Figure~\ref{fig:polarization} traces the polarization index across our seven Congresses. The pattern is non-monotonic. Polarization begins at a moderate level in the 110th Congress (0.520), drops to its lowest point in the 111th (0.400), then jumps sharply to 0.647 in the 112th, a 62 percent increase. It continues rising through the 113th (0.640) and 114th (0.690), before stabilizing at roughly 0.640 for the 115th and 116th Congresses.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../figures/fig1_polarization.pdf}
\caption{Polarization index across the 110th--116th Congresses, measured as $1 - (\text{between-party agreement} / \text{within-party agreement})$. The Tea Party wave of 2010 produces the largest single-session jump.}
\label{fig:polarization}
\end{figure}

The dip in the 111th Congress is consistent with the large Democratic majority (269--183) elected alongside Obama in 2008. With a comfortable margin, the majority party had less need for strict discipline, and many Blue Dog Democrats voted with Republicans on fiscal issues. The subsequent spike aligns precisely with the Tea Party wave: the 2010 midterms replaced many moderate Democrats with conservative Republicans and, perhaps more importantly, created strong incentives for surviving members of both parties to toe the party line.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{../figures/fig2_ideology_space.pdf}
\caption{House members in DW-NOMINATE ideological space for three Congresses. The shrinking overlap between parties is visible in the shift from the 110th to the 116th Congress.}
\label{fig:ideology}
\end{figure}

% ============================================================
\section{Interpretability: What Does the Model Learn?}
% ============================================================

The attention mechanism in CongressGAT is not a black box. At each layer, the model computes attention weights $\alpha_{ij}$ for every pair of connected nodes, and these weights tell us how much information node $i$ draws from node $j$. By aggregating these weights by party alignment, we can ask a simple question: does the model pay more attention to same-party relationships or cross-party ones?

\subsection{Attention Weight Analysis}

Table~\ref{tab:attention} reports the mean attention weight for same-party and cross-party edges, along with their ratio, for each Congress. The pattern is striking. In the earlier, less polarized Congresses (110th--113th), the model assigns modestly higher attention to same-party edges (ratios of 1.07--1.13). But in the later, more polarized sessions (114th--116th), the pattern reverses: cross-party edges receive \textit{more} attention than same-party ones (ratios of 0.85--0.90).

\begin{table}[H]
\centering
\caption{GAT Attention Weights by Party Alignment}
\label{tab:attention}
\small
\begin{tabular}{lccc}
\toprule
Congress & Same-Party & Cross-Party & Ratio (Same/Cross) \\
\midrule
110th & 0.00368 & 0.00325 & 1.131 \\
111th & 0.00260 & 0.00230 & 1.130 \\
112th & 0.00421 & 0.00381 & 1.104 \\
113th & 0.00437 & 0.00407 & 1.074 \\
114th & 0.00441 & 0.00490 & 0.899 \\
115th & 0.00423 & 0.00495 & 0.855 \\
116th & 0.00417 & 0.00488 & 0.855 \\
\bottomrule
\end{tabular}
\medskip

\footnotesize{Attention weights are averaged across all edges in the second GAT layer. Ratio above 1 means the model attends more to co-partisans; below 1 means more to cross-party links.}
\end{table}

This makes intuitive sense if you think about what the model is trying to do. In a less polarized Congress, where many members cross party lines, within-party relationships are more informative because they distinguish the party loyalists from the moderates. In a highly polarized Congress, where nearly everyone votes with their party, the few remaining cross-party connections become the rare, informative signal. The model learns to attend to whatever is scarce and discriminating.

This finding also provides a natural check on the model's behavior. If the attention mechanism were simply memorizing party labels, we would expect same-party attention to dominate everywhere. The fact that the pattern reverses in more polarized sessions suggests the model is learning something about the structure of the network, not just reproducing a party classifier.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{../figures/fig5_attention_analysis.pdf}
\caption{Left: Mean attention weights by party alignment. Right: The ratio of same-party to cross-party attention declines as polarization increases, falling below 1.0 in the 114th--116th Congresses.}
\label{fig:attention}
\end{figure}

\subsection{Feature Importance}

The random forest baselines offer a complementary view of feature importance through Gini impurity scores (Figure~\ref{fig:features}). For coalition detection, DW-NOMINATE's first dimension dominates, contributing more than 60 percent of the predictive power. For defection prediction, the network-derived features (mean same-party and cross-party agreement) are the most important, confirming that the relational structure of the voting network, not just individual ideology, drives defection behavior.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{../figures/fig7_feature_importance.pdf}
\caption{Random forest feature importance for coalition detection (left) and defection prediction (right). Network-derived agreement features dominate defection prediction.}
\label{fig:features}
\end{figure}

% ============================================================
\section{Quasi-Causal Analysis}
% ============================================================

Can we attribute observed polarization shifts to specific political events? The challenge is the standard one in observational studies: we cannot observe the counterfactual. We cannot know what the 112th Congress would have looked like without the Tea Party wave. But we can use the temporal structure of our data to construct a difference-in-differences style estimate, comparing observed shifts against the pre-treatment trend.

\subsection{The Tea Party Shock}

The 2010 midterm elections are the cleanest natural experiment in our sample. The Republican Party gained 63 House seats, the largest midterm gain since 1938, and many of the new members ran explicitly on Tea Party platforms of fiscal conservatism and government reduction.

The pre-treatment trend from the 110th to 111th Congress was actually a \textit{decrease} in polarization ($\Delta = -0.120$), as the large incoming Democratic class included many moderates from swing districts. The treatment period shift from the 111th to 112th was an increase of 0.247. Subtracting the pre-trend yields an estimated Tea Party effect of $+0.367$, though this estimate should be interpreted cautiously given the obvious limitations of two-period differencing with a single treated unit.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{../figures/fig6_causal_analysis.pdf}
\caption{Left: Polarization before and after the Tea Party wave (2010). Right: Polarization during the Trump era. The Tea Party produced a larger discrete shift; the Trump era saw a slight \textit{decrease} relative to the pre-trend.}
\label{fig:causal}
\end{figure}

The freshman class analysis reinforces this finding. New Republican members entering the 112th Congress had a mean DW-NOMINATE score notably to the right of returning Republicans, consistent with the incoming Tea Party cohort pulling the caucus further from the center.

\subsection{The Trump Era}

The Trump era tells a different story. Polarization actually decreased slightly from the 114th to the 115th Congress ($\Delta = -0.050$), and the pre-trend from the 113th to 114th was an increase of 0.050. The estimated Trump-era ``effect'' is therefore $-0.100$: if anything, the onset of the Trump presidency was associated with a slight moderation in our polarization measure, though the magnitude is small enough to be noise.

This finding may seem counterintuitive given the popular narrative of Trump-era polarization, but it is consistent with several observations. First, much of the perceived increase in political conflict during the Trump years occurred outside the roll-call voting context, in floor speeches, media appearances, and social media rather than in recorded votes. Second, the 115th Congress included several high-profile bipartisan votes (disaster relief, the First Step Act) that may have pushed up the between-party agreement rate. Third, our measure captures voting behavior, not rhetorical extremism, and these can diverge.

\subsection{Defection Dynamics}

The trajectory of party defection rates provides additional texture. Republican defection rates peak in the 112th Congress at the same time polarization spikes, a pattern consistent with the Tea Party insurgency creating within-party conflict as establishment Republicans clashed with the incoming cohort. Democratic defection rates, by contrast, show a steady decline from the 110th through the 115th Congress, reflecting the gradual extinction of the Blue Dog coalition as moderate Democrats lost their seats in successive wave elections.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{../figures/fig4_defection_analysis.pdf}
\caption{Left: Party defection rates over time. Republican defection peaks during the Tea Party era (112th); Democratic defection declines steadily. Right: Polarization by Congress, with train/test split indicated.}
\label{fig:defection}
\end{figure}

The 115th Congress marks the low point for defection on both sides: only 6.7 percent of members exceeded our 10 percent threshold. This is the Congress with the narrowest band of bipartisan voting in our sample, and it coincides with a period when both parties faced strong incentives for unity, Republicans seeking to advance the Trump agenda and Democrats seeking to oppose it.

% ============================================================
\section{The 114th Congress Anomaly}
% ============================================================

The 114th Congress (2015--2017) presents a puzzle. It has the highest polarization index in our sample (0.690) and the second-fewest defectors (39), yet its spectral clustering accuracy is only 0.544, far below the 0.99+ achieved by most other Congresses. Spectral clustering uses the Fiedler vector of the graph Laplacian to partition the network into two groups; an accuracy near 0.5 means the natural graph partition does not align with party labels.

What explains this discrepancy? High polarization means that between-party agreement is low relative to within-party agreement. But spectral clustering depends on the absolute structure of the graph, not just the relative rates. The 114th Congress, led by Speaker Paul Ryan after John Boehner's resignation, was marked by deep intra-party divisions within the Republican Conference. The Freedom Caucus, a block of 30--40 hardline conservatives, regularly voted against leadership priorities, creating a within-party cleavage that was sometimes as sharp as the between-party divide.

The spectral clustering algorithm picks up this within-party fragmentation. When the dominant two-way split in the network is not Democrat-vs.-Republican but establishment-vs.-insurgent, spectral methods assign members to clusters that cross party lines. The polarization index, which only compares within-party to between-party agreement rates, misses this internal structure.

This is precisely the kind of finding that graph-based methods are designed to surface. A scalar polarization measure says the 114th is the most polarized Congress in the sample. A graph-based analysis reveals that the 114th's apparent polarization coexists with unusual internal fragmentation, a qualitatively different kind of dysfunction than the clean partisan sorting of the 115th or 116th.

% ============================================================
\section{Discussion}
% ============================================================

What does CongressGAT actually learn? The honest answer is: less than we initially hoped, and something different from what we expected.

On pure prediction, the model does not outperform well-engineered baselines. Logistic regression with hand-crafted network features beats the GAT on defection prediction by a substantial margin (AUC of 0.986 vs. 0.799). The lesson is not that graph neural networks are unsuitable for legislative data, but that the informational advantage of learning from the full graph structure, as opposed to simple network summaries, is modest when the relevant network information is low-dimensional. Two summary statistics, mean same-party agreement and mean cross-party agreement, capture most of what the model needs to predict defection.

But prediction is not the only goal, and it is not where the graph approach adds the most value. The attention mechanism provides a form of interpretability that neither regression coefficients nor feature importance scores can match. The finding that the model shifts from same-party to cross-party attention as polarization increases is a genuinely informative result: it tells us that the model identifies bipartisan connections as the rare, high-signal feature in a polarized environment. This is consistent with the theoretical intuition that information lives at the boundaries, and it is not something that any baseline model reveals.

The polarization prediction failure is also informative, though in a negative sense. With only seven congress-level observations, no model can learn a reliable regression function. This is a hard constraint of the domain, not a model limitation, and it suggests that temporal models of congressional polarization need to operate at a finer temporal resolution (within-session dynamics, for instance) to have enough data for meaningful prediction.

\subsection{Limitations}

Several limitations deserve acknowledgment. First, our node features are minimal: two DW-NOMINATE dimensions and a party indicator. Richer features, including committee assignments, tenure, electoral margin, and demographic characteristics, would likely improve all models, though they would also raise questions about what the graph structure contributes beyond what features already encode.

Second, our sample covers only seven Congresses. Extending the analysis backward (to the 100th or earlier) would provide more temporal variation, though it would also introduce structural changes in the institution that complicate comparisons.

Third, the causal analysis is suggestive, not definitive. A difference-in-differences estimate with a single treated unit and no control group is, at best, a disciplined description of a temporal shift. We describe it as ``quasi-causal'' advisedly.

Fourth, we treat the voting agreement graph as undirected and do not model the content of the votes themselves. A richer model might distinguish between agreement on procedural votes (which tend to be party-line) and agreement on substantive legislation (which sometimes crosses party lines), potentially revealing a different structure of bipartisan cooperation.

% ============================================================
\section{Conclusion}
% ============================================================

The question we began with was whether graph-based methods could capture dimensions of congressional polarization that standard approaches miss. The answer is a qualified yes. The graph attention network does not predict better than engineered baselines, but it reveals structural patterns, the shift in attention from same-party to cross-party edges, the within-party fragmentation of the 114th Congress, that scalar measures cannot surface. And the approach confirms, with real data and proper out-of-sample evaluation, the Tea Party wave of 2010 as the dominant discrete shock to House polarization in the 2007--2021 period.

The broader point is about the kind of question that graph methods are suited to answer. They are not (yet) a superior predictive tool for legislative behavior, a domain where simple features go a long way. They are a tool for structural discovery, for asking how the network of legislative relationships is organized and how that organization changes. In a Congress where the defining feature of political life is the relationship between members, not their individual positions, that seems like a question worth asking.

\newpage
\bibliographystyle{apalike}

\begin{thebibliography}{99}

\bibitem[Andris et al., 2015]{andris2015rise}
Andris, C., Lee, D., Hamilton, M. J., Martino, M., Gunning, C. E., \& Selden, J. A. (2015). The rise of partisanship and super-cooperators in the U.S. House of Representatives. \textit{PLoS ONE}, 10(4), e0123507.

\bibitem[Fiorina et al., 2005]{fiorina2005culture}
Fiorina, M. P., Abrams, S. J., \& Pope, J. C. (2005). \textit{Culture War? The Myth of a Polarized America}. Pearson Longman.

\bibitem[Hall, 2015]{hall2015happens}
Hall, A. B. (2015). What happens when extremists win primaries? \textit{American Political Science Review}, 109(1), 18--42.

\bibitem[Kipf \& Welling, 2017]{kipf2017semi}
Kipf, T. N., \& Welling, M. (2017). Semi-supervised classification with graph convolutional networks. \textit{Proceedings of ICLR 2017}.

\bibitem[Lee, 2009]{lee2009beyond}
Lee, F. E. (2009). \textit{Beyond Ideology: Politics, Principles, and Partisanship in the U.S. Senate}. University of Chicago Press.

\bibitem[Lewis et al., 2023]{lewis2023voteview}
Lewis, J. B., Poole, K., Rosenthal, H., Boche, A., Rudkin, A., \& Sonnet, L. (2023). Voteview: Congressional roll-call votes database. \url{https://voteview.com/}.

\bibitem[McCarty et al., 2006]{mccarty2006polarized}
McCarty, N., Poole, K. T., \& Rosenthal, H. (2006). \textit{Polarized America: The Dance of Ideology and Unequal Riches}. MIT Press.

\bibitem[Poole \& Rosenthal, 1997]{poole1997congress}
Poole, K. T., \& Rosenthal, H. (1997). \textit{Congress: A Political-Economic History of Roll Call Voting}. Oxford University Press.

\bibitem[Vaswani et al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., \& Polosukhin, I. (2017). Attention is all you need. \textit{Advances in Neural Information Processing Systems}, 30.

\bibitem[Veli\v{c}kovi\'{c} et al., 2018]{velickovic2018graph}
Veli\v{c}kovi\'{c}, P., Cucurull, G., Casanova, A., Romero, A., Li\`{o}, P., \& Bengio, Y. (2018). Graph attention networks. \textit{Proceedings of ICLR 2018}.

\bibitem[Wu et al., 2020]{wu2020comprehensive}
Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., \& Yu, P. S. (2020). A comprehensive survey on graph neural networks. \textit{IEEE Transactions on Neural Networks and Learning Systems}, 32(1), 4--24.

\end{thebibliography}

\newpage
\appendix

\section{Model Hyperparameters}

\begin{table}[H]
\centering
\small
\begin{tabular}{ll}
\toprule
Parameter & Value \\
\midrule
GAT hidden dimension & 32 \\
GAT attention heads (layer 1) & 4 \\
GAT attention heads (layer 2) & 1 \\
Temporal attention heads & 2 \\
Dropout & 0.3 \\
Learning rate & 0.005 \\
Weight decay & $10^{-4}$ \\
Max epochs & 300 \\
Early stopping patience & 30 \\
Gradient clip norm & 1.0 \\
Agreement threshold (edges) & 0.5 \\
Defection threshold & 10\% \\
\bottomrule
\end{tabular}
\caption{CongressGAT hyperparameters.}
\end{table}

\section{Per-Congress Results}

\begin{table}[H]
\centering
\small
\begin{tabular}{lcccc}
\toprule
Congress & Split & Coalition F1 & Defection AUC & Polarization \\
\midrule
110th & Train & 0.895 & 0.815 & 0.520 \\
111th & Train & 0.373 & 0.660 & 0.400 \\
112th & Train & 0.995 & 0.668 & 0.647 \\
113th & Train & 1.000 & 0.661 & 0.640 \\
114th & Train & 0.998 & 0.708 & 0.690 \\
115th & Test & 0.996 & 0.669 & 0.640 \\
116th & Test & 0.984 & 0.816 & 0.642 \\
\bottomrule
\end{tabular}
\caption{CongressGAT performance by Congress. The 111th Congress shows notably lower coalition F1, likely because it is the least polarized session and the graph structure is less cleanly partisan.}
\end{table}

\end{document}
